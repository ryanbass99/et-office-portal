import fs from "fs";
import path from "path";
import admin from "firebase-admin";
import { parse } from "csv-parse";

// =====================
// CONFIG (EDIT THESE)
// =====================
const SERVICE_ACCOUNT_PATH = "C:\\sageexports\\serviceAccountKey.json";
const INV_HH_CSV_PATH = "C:\\sageexports\\Inv_HH.csv";
const INV_HD_CSV_PATH = "C:\\sageexports\\Inv_HD.csv";

// If InvoiceDate is weird, we try multiple formats below.
const DAYS_BACK = 365;

// Firestore collection root
const INVOICES_COLLECTION = "invoices";

// BulkWriter tuning
const BULK_FLUSH_EVERY = 5000; // flush every N operations

// =====================
// INIT FIREBASE ADMIN
// =====================
const serviceAccount = JSON.parse(fs.readFileSync(SERVICE_ACCOUNT_PATH, "utf8"));

admin.initializeApp({
  credential: admin.credential.cert(serviceAccount),
});

const db = admin.firestore();

// =====================
// HELPERS
// =====================
function cleanStr(v) {
  if (v === null || v === undefined) return "";
  return String(v).trim();
}

function normalizeDocId(invoiceNo) {
  // Just in case there are odd characters
  return cleanStr(invoiceNo).replaceAll("/", "-");
}

/**
 * Parse InvoiceDate into a JS Date.
 * Supports:
 * - ISO: 2025-02-01, 2025-02-01T00:00:00
 * - US: 2/1/2025 or 02/01/2025
 * - US with time: 02/01/2025 13:05:00
 */
function parseInvoiceDate(raw) {
  const s = cleanStr(raw);
  if (!s) return null;

  // ISO-ish
  const iso = Date.parse(s);
  if (!Number.isNaN(iso)) return new Date(iso);

  // Try MM/DD/YYYY or M/D/YYYY (optionally with time)
  const m = s.match(/^(\d{1,2})\/(\d{1,2})\/(\d{2,4})(?:\s+(\d{1,2}):(\d{2})(?::(\d{2}))?)?$/);
  if (m) {
    const mm = Number(m[1]);
    const dd = Number(m[2]);
    let yyyy = Number(m[3]);
    if (yyyy < 100) yyyy += 2000;
    const hh = m[4] ? Number(m[4]) : 0;
    const mi = m[5] ? Number(m[5]) : 0;
    const ss = m[6] ? Number(m[6]) : 0;
    // Local time is fine for “last 365 days” cutoff
    return new Date(yyyy, mm - 1, dd, hh, mi, ss);
  }

  return null;
}

function daysAgoDate(days) {
  const d = new Date();
  d.setDate(d.getDate() - days);
  return d;
}

function toISODateOnly(d) {
  // YYYY-MM-DD
  const yyyy = d.getFullYear();
  const mm = String(d.getMonth() + 1).padStart(2, "0");
  const dd = String(d.getDate()).padStart(2, "0");
  return `${yyyy}-${mm}-${dd}`;
}

async function streamCsv(filePath, onRow) {
  return new Promise((resolve, reject) => {
    const parser = parse({
      columns: true,
      bom: true,
      relax_quotes: true,
      relax_column_count: true,
      skip_empty_lines: true,
      trim: true,
    });

    let rowCount = 0;

    fs.createReadStream(filePath)
      .on("error", reject)
      .pipe(parser)
      .on("data", async (row) => {
        parser.pause();
        try {
          rowCount++;
          await onRow(row, rowCount);
        } catch (err) {
          reject(err);
          return;
        } finally {
          parser.resume();
        }
      })
      .on("end", () => resolve(rowCount))
      .on("error", reject);
  });
}

// =====================
// MAIN IMPORT
// =====================
async function run() {
  const cutoff = daysAgoDate(DAYS_BACK);
  console.log(`Cutoff (>=): ${cutoff.toString()}  | dateOnly: ${toISODateOnly(cutoff)}`);

  // Track eligible invoices (last year)
  const eligibleInvoiceNos = new Set();

  // BulkWriter for speed + retries
  const writer = db.bulkWriter();
  let opsSinceFlush = 0;

  writer.onWriteError((err) => {
    // Retry transient errors a few times
    if (err.failedAttempts < 5) {
      return true;
    }
    console.error("Write failed permanently:", err);
    return false;
  });

  // -------- PASS 1: HEADERS --------
  console.log(`\nPASS 1: Reading headers: ${INV_HH_CSV_PATH}`);

  let headersKept = 0;
  let headersSkipped = 0;
  let headersBadDate = 0;

  await streamCsv(INV_HH_CSV_PATH, async (row, i) => {
    const invoiceNo = cleanStr(row.InvoiceNo);
    const invoiceDateRaw = row.InvoiceDate;

    const invoiceDate = parseInvoiceDate(invoiceDateRaw);
    if (!invoiceDate) {
      headersBadDate++;
      if (headersBadDate <= 5) {
        console.warn(`Bad InvoiceDate at row ${i}:`, invoiceDateRaw);
      }
      return;
    }

    if (invoiceDate < cutoff) {
      headersSkipped++;
      return;
    }

    headersKept++;
    eligibleInvoiceNos.add(invoiceNo);

    const docId = normalizeDocId(invoiceNo);
    const ref = db.collection(INVOICES_COLLECTION).doc(docId);

    // Store both a Date (for easy display) + timestamp
    writer.set(
      ref,
      {
        invoiceNo,
        invoiceDate: admin.firestore.Timestamp.fromDate(invoiceDate),
        invoiceDateRaw: cleanStr(invoiceDateRaw),
        customerNo: cleanStr(row.CustomerNo),
        salespersonNo: cleanStr(row.SalespersonNo),
        importedAt: admin.firestore.FieldValue.serverTimestamp(),
      },
      { merge: true }
    );

    opsSinceFlush++;
    if (opsSinceFlush >= BULK_FLUSH_EVERY) {
      await writer.flush();
      opsSinceFlush = 0;
      console.log(`Headers progress: processed ${i}, kept ${headersKept}, skipped ${headersSkipped}, eligibleSet=${eligibleInvoiceNos.size}`);
    }
  });

  await writer.flush();

  console.log(`PASS 1 done. kept=${headersKept}, skipped(old)=${headersSkipped}, badDate=${headersBadDate}, eligibleSet=${eligibleInvoiceNos.size}`);

  // -------- PASS 2: LINES --------
  console.log(`\nPASS 2: Reading lines: ${INV_HD_CSV_PATH}`);

  let linesKept = 0;
  let linesSkipped = 0;

  let lineOpsSinceFlush = 0;

  await streamCsv(INV_HD_CSV_PATH, async (row, i) => {
    const invoiceNo = cleanStr(row.InvoiceNo);
    if (!eligibleInvoiceNos.has(invoiceNo)) {
      linesSkipped++;
      return;
    }

    linesKept++;

    const docId = normalizeDocId(invoiceNo);
    const parentRef = db.collection(INVOICES_COLLECTION).doc(docId);

    // Subcollection under invoice
    const lineRef = parentRef.collection("lines").doc(); // autoId
    writer.set(lineRef, {
      invoiceNo,
      itemCode: cleanStr(row.ItemCode),
      itemCodeDesc: cleanStr(row.ItemCodeDesc),
      quantityShipped: Number(row.QuantityShipped || 0),
      importedAt: admin.firestore.FieldValue.serverTimestamp(),
    });

    lineOpsSinceFlush++;
    if (lineOpsSinceFlush >= BULK_FLUSH_EVERY) {
      await writer.flush();
      lineOpsSinceFlush = 0;
      console.log(`Lines progress: processed ${i}, kept ${linesKept}, skipped ${linesSkipped}`);
    }
  });

  await writer.flush();
  await writer.close();

  console.log(`PASS 2 done. kept=${linesKept}, skipped(notEligible)=${linesSkipped}`);
  console.log("\n✅ Import complete.");
}

run().catch((err) => {
  console.error("Import failed:", err);
  process.exit(1);
});
